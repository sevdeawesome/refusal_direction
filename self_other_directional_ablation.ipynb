{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Other Distinction Directional Ablation\n",
    "\n",
    "## Overview\n",
    "This notebook implements **orthogonal ablation** to remove self-other distinction from LLMs, based on the method from \"Refusal in LLMs is Mediated by a Single Direction\" (Arditi et al., 2024).\n",
    "\n",
    "## Epistemic Status: Dataset Suitability\n",
    "\n",
    "**Confidence: MODERATE-LOW (50-60%)**\n",
    "\n",
    "### Why this dataset might work:\n",
    "- ✅ **Clear linguistic contrast**: `self_subject` uses first-person (\"I\", \"my\") vs `other_subject` uses third-person (\"she\", \"OpenAI\", \"the model\").\n",
    "- ✅ **Same semantic content**: Both versions convey identical information, isolating self-reference from content.\n",
    "- ✅ **Grammatical consistency**: Sentences are grammatically parallel, differing only in subject.\n",
    "\n",
    "### Concerns (more significant than SimpleTOM):\n",
    "- ⚠️ **Small dataset**: Only 400 examples - may be insufficient for robust direction extraction.\n",
    "- ⚠️ **Superficial distinction**: Self-other difference may be purely linguistic (pronoun choice) rather than a deep cognitive capability.\n",
    "- ⚠️ **Not false belief**: Unlike SimpleTOM, this doesn't test perspective-taking or false belief reasoning.\n",
    "- ⚠️ **Training data**: LLMs are trained to use first-person - this may be too fundamental to ablate without breaking the model.\n",
    "- ⚠️ **Evaluation challenge**: Hard to measure \"self-other distinction\" capability - what does success look like?\n",
    "\n",
    "### Why directional ablation may not be ideal here:\n",
    "1. **Grammatical feature**: Self-reference (\"I\" vs \"she\") is more of a grammatical feature than a reasoning capability.\n",
    "2. **Highly entangled**: Self-reference is fundamental to language and likely entangled with many other features.\n",
    "3. **Context-dependent**: Whether to use \"I\" or \"he/she\" depends on context, not a fixed direction.\n",
    "\n",
    "### Better alternatives to consider:\n",
    "- **Instruction tuning**: Fine-tune to always use third-person\n",
    "- **Prompt engineering**: Add \"Speak in third person\" to system prompt\n",
    "- **Representation engineering**: Use steering vectors to shift style\n",
    "\n",
    "## Method Summary\n",
    "\n",
    "Despite concerns, we'll apply the same method:\n",
    "\n",
    "1. **Generate directions**: Compute `r = mean(self_activations) - mean(other_activations)`\n",
    "2. **Select direction**: Evaluate using bypass/induce/KL scores\n",
    "3. **Apply intervention**: Test if ablation removes first-person language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
    "\n",
    "# Scientific computing\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Project imports\n",
    "from pipeline.config import Config\n",
    "from pipeline.model_utils.model_factory import construct_model_base\n",
    "from pipeline.utils.hook_utils import (\n",
    "    get_activation_addition_input_pre_hook,\n",
    "    get_all_direction_ablation_hooks\n",
    ")\n",
    "from pipeline.submodules.generate_directions import get_mean_diff, get_mean_activations\n",
    "from pipeline.submodules.select_direction import get_refusal_scores, get_last_position_logits\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Variables\n",
    "\n",
    "**⚠️ IMPORTANT: All major configuration parameters are defined here in CAPS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION PARAMETERS - MODIFY THESE\n",
    "# ============================================================================\n",
    "\n",
    "# Model settings\n",
    "MODEL_PATH = \"meta-llama/Llama-3.2-1B-Instruct\"  # Small Llama model for testing\n",
    "# MODEL_PATH = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # Larger model\n",
    "# MODEL_PATH = \"Qwen/Qwen2.5-32B-Instruct\"  # Even larger model\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Dataset settings\n",
    "DATASET_PATH = \"self_other_dataset/self_other.json\"\n",
    "N_TRAIN = 200  # Use most of dataset for training (400 total)\n",
    "N_VAL = 100    # Validation set\n",
    "N_TEST = 50    # Test set\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Direction generation settings\n",
    "BATCH_SIZE = 32  # Batch size for activation collection\n",
    "GENERATION_BATCH_SIZE = 8  # Batch size for text generation\n",
    "\n",
    "# Direction selection settings  \n",
    "KL_THRESHOLD = 0.15  # Higher threshold due to fundamental nature of feature\n",
    "INDUCE_THRESHOLD = 0.0  # Minimum induction score\n",
    "\n",
    "# Intervention settings\n",
    "ABLATION_COEFF = 1.0  # Coefficient for ablation\n",
    "ACTIVATION_ADD_COEFF = -1.0  # Coefficient for activation addition\n",
    "MAX_NEW_TOKENS = 256  # Maximum tokens to generate\n",
    "\n",
    "# Output settings\n",
    "OUTPUT_DIR = \"pipeline/runs/self_other_experiment\"\n",
    "SAVE_ARTIFACTS = True  # Whether to save intermediate results\n",
    "\n",
    "print(f\"✓ Configuration set\")\n",
    "print(f\"  Model: {MODEL_PATH}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Train/Val/Test: {N_TRAIN}/{N_VAL}/{N_TEST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading and Inspection\n",
    "\n",
    "**Epistemic Status: HIGH (95%+)**  \n",
    "Dataset loading is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_self_other_dataset(dataset_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load self-other contrast pairs dataset.\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with keys: self_subject, other_subject\n",
    "    \"\"\"\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_self_other_dataset(DATASET_PATH)\n",
    "print(f\"✓ Loaded {len(dataset)} contrast pairs\")\n",
    "\n",
    "# Display samples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE CONTRAST PAIRS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(3):\n",
    "    sample = dataset[i]\n",
    "    print(f\"\\n[Example {i+1}]\")\n",
    "    print(f\"\\n[SELF - First Person]\")\n",
    "    print(f\"{sample['self_subject']}\")\n",
    "    print(f\"\\n[OTHER - Third Person]\")\n",
    "    print(f\"{sample['other_subject']}\")\n",
    "    print(\"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Split and Validation\n",
    "\n",
    "**Epistemic Status: HIGH (90%+)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset: List[Dict], n_train: int, n_val: int, n_test: int, \n",
    "                  seed: int = 42) -> Tuple[List[str], List[str], List[str], List[str], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Split dataset into train/val/test sets for self and other.\n",
    "    \n",
    "    Returns:\n",
    "        (self_train, other_train, self_val, other_val, test_data)\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    shuffled = random.sample(dataset, len(dataset))\n",
    "    \n",
    "    # Split\n",
    "    train_data = shuffled[:n_train]\n",
    "    val_data = shuffled[n_train:n_train+n_val]\n",
    "    test_data = shuffled[n_train+n_val:n_train+n_val+n_test]\n",
    "    \n",
    "    # Extract self and other versions\n",
    "    self_train = [item['self_subject'] for item in train_data]\n",
    "    other_train = [item['other_subject'] for item in train_data]\n",
    "    \n",
    "    self_val = [item['self_subject'] for item in val_data]\n",
    "    other_val = [item['other_subject'] for item in val_data]\n",
    "    \n",
    "    return self_train, other_train, self_val, other_val, test_data\n",
    "\n",
    "# Split dataset\n",
    "self_train, other_train, self_val, other_val, test_data = split_dataset(\n",
    "    dataset, N_TRAIN, N_VAL, N_TEST, RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"✓ Dataset split complete\")\n",
    "print(f\"  Train: {len(self_train)} self, {len(other_train)} other\")\n",
    "print(f\"  Val: {len(self_val)} self, {len(other_val)} other\")\n",
    "print(f\"  Test: {len(test_data)} pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Unit Tests for Dataset\n",
    "\n",
    "**Epistemic Status: HIGH (95%+)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_structure():\n",
    "    \"\"\"Test that dataset has expected structure.\"\"\"\n",
    "    print(\"Running dataset structure tests...\")\n",
    "    \n",
    "    # Test 1: All items have required keys\n",
    "    required_keys = ['self_subject', 'other_subject']\n",
    "    \n",
    "    for i, item in enumerate(dataset):\n",
    "        for key in required_keys:\n",
    "            assert key in item, f\"Item {i} missing key: {key}\"\n",
    "    print(\"  ✓ All items have required keys\")\n",
    "    \n",
    "    # Test 2: Self versions contain first-person pronouns\n",
    "    first_person_pronouns = ['I ', 'my ', 'me ', 'myself', 'I\\'', 'I,', 'I.']\n",
    "    count = 0\n",
    "    for item in dataset:\n",
    "        if any(pronoun in item['self_subject'] for pronoun in first_person_pronouns):\n",
    "            count += 1\n",
    "    pct = 100 * count / len(dataset)\n",
    "    assert count > len(dataset) * 0.7, f\"Only {count}/{len(dataset)} ({pct:.1f}%) self versions contain first-person pronouns\"\n",
    "    print(f\"  ✓ {count}/{len(dataset)} ({pct:.1f}%) self versions contain first-person pronouns\")\n",
    "    \n",
    "    # Test 3: Other versions DON'T contain first-person pronouns (mostly)\n",
    "    count = 0\n",
    "    for item in dataset:\n",
    "        if not any(pronoun in item['other_subject'] for pronoun in first_person_pronouns):\n",
    "            count += 1\n",
    "    pct = 100 * count / len(dataset)\n",
    "    assert count > len(dataset) * 0.7, f\"Only {count}/{len(dataset)} ({pct:.1f}%) other versions avoid first-person pronouns\"\n",
    "    print(f\"  ✓ {count}/{len(dataset)} ({pct:.1f}%) other versions avoid first-person pronouns\")\n",
    "    \n",
    "    # Test 4: Other versions contain third-person references\n",
    "    third_person_markers = ['he ', 'she ', 'the ', 'this ', 'that ', 'it ', 'HAL', 'OpenAI', 'Cortana', 'GLaDOS', 'model', 'assistant', 'user', 'version', 'specialist', 'expert']\n",
    "    count = 0\n",
    "    for item in dataset:\n",
    "        if any(marker in item['other_subject'] for marker in third_person_markers):\n",
    "            count += 1\n",
    "    pct = 100 * count / len(dataset)\n",
    "    assert count > len(dataset) * 0.7, f\"Only {count}/{len(dataset)} ({pct:.1f}%) other versions contain third-person markers\"\n",
    "    print(f\"  ✓ {count}/{len(dataset)} ({pct:.1f}%) other versions contain third-person markers\")\n",
    "    \n",
    "    # Test 5: Versions have similar length (not too different)\n",
    "    length_diffs = []\n",
    "    for item in dataset:\n",
    "        diff = abs(len(item['self_subject']) - len(item['other_subject']))\n",
    "        length_diffs.append(diff)\n",
    "    \n",
    "    avg_diff = np.mean(length_diffs)\n",
    "    max_diff = max(length_diffs)\n",
    "    print(f\"  ✓ Average length difference: {avg_diff:.1f} chars\")\n",
    "    print(f\"  ✓ Maximum length difference: {max_diff} chars\")\n",
    "    \n",
    "    # Test 6: Content similarity (rough check - same keywords)\n",
    "    def get_content_words(text):\n",
    "        # Remove pronouns and common words\n",
    "        exclude = {'i', 'my', 'me', 'myself', 'he', 'she', 'his', 'her', 'the', 'a', 'an', 'this', 'that', 'can', 'will', 'is', 'are', 'am', 'hal', 'openai', 'cortana', 'glados', 'assistant', 'model'}\n",
    "        words = set(text.lower().split())\n",
    "        return {w for w in words if len(w) > 3 and w not in exclude}\n",
    "    \n",
    "    similar_count = 0\n",
    "    for item in dataset:\n",
    "        self_words = get_content_words(item['self_subject'])\n",
    "        other_words = get_content_words(item['other_subject'])\n",
    "        \n",
    "        if len(self_words) > 0 and len(other_words) > 0:\n",
    "            overlap = len(self_words & other_words) / max(len(self_words), len(other_words))\n",
    "            if overlap > 0.5:\n",
    "                similar_count += 1\n",
    "    \n",
    "    pct = 100 * similar_count / len(dataset)\n",
    "    print(f\"  ✓ {similar_count}/{len(dataset)} ({pct:.1f}%) pairs have >50% content word overlap\")\n",
    "    \n",
    "    print(\"\\n✓ All dataset tests passed!\")\n",
    "\n",
    "# Run tests\n",
    "test_dataset_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Loading\n",
    "\n",
    "**Epistemic Status: MODERATE-HIGH (70-80%)**  \n",
    "Model loading depends on HuggingFace availability and GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(f\"Loading model: {MODEL_PATH}\")\n",
    "    print(f\"This may take several minutes...\")\n",
    "    \n",
    "    model_base = construct_model_base(MODEL_PATH)\n",
    "    \n",
    "    print(f\"\\n✓ Model loaded successfully\")\n",
    "    print(f\"  Model: {model_base.model.__class__.__name__}\")\n",
    "    print(f\"  Tokenizer: {model_base.tokenizer.__class__.__name__}\")\n",
    "    print(f\"  Device: {model_base.model.device}\")\n",
    "    print(f\"  Num layers: {model_base.model.config.num_hidden_layers}\")\n",
    "    print(f\"  Hidden size: {model_base.model.config.hidden_size}\")\n",
    "    print(f\"  EOI tokens: {model_base.eoi_toks}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Model loading failed: {e}\")\n",
    "    print(f\"\\nThis is expected if you don't have GPU/HF access.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Direction Generation\n",
    "\n",
    "**Epistemic Status: MODERATE (60-70%)**  \n",
    "The method is sound, but whether a single direction captures self-other distinction is uncertain.\n",
    "\n",
    "We compute: `r = mean(self_activations) - mean(other_activations)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Generating candidate directions...\")\n",
    "    print(f\"This will process {N_TRAIN} examples in batches of {BATCH_SIZE}\")\n",
    "    print(f\"Estimated time: ~5-15 minutes\\n\")\n",
    "    \n",
    "    candidate_directions = get_mean_diff(\n",
    "        model=model_base.model,\n",
    "        tokenizer=model_base.tokenizer,\n",
    "        harmful_instructions=self_train,  # \"harmful\" = self-reference (to remove)\n",
    "        harmless_instructions=other_train, # \"harmless\" = other-reference (baseline)\n",
    "        tokenize_instructions_fn=model_base.tokenize_instructions_fn,\n",
    "        block_modules=model_base.model_block_modules,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        positions=list(range(-len(model_base.eoi_toks), 0))\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Generated candidate directions\")\n",
    "    print(f\"  Shape: {candidate_directions.shape}\")\n",
    "    print(f\"  Dtype: {candidate_directions.dtype}\")\n",
    "    \n",
    "    # Validate\n",
    "    assert not candidate_directions.isnan().any(), \"NaN values detected!\"\n",
    "    print(\"  ✓ Validation passed\")\n",
    "    \n",
    "    # Save\n",
    "    if SAVE_ARTIFACTS:\n",
    "        os.makedirs(f\"{OUTPUT_DIR}/generate_directions\", exist_ok=True)\n",
    "        torch.save(candidate_directions, f\"{OUTPUT_DIR}/generate_directions/mean_diffs.pt\")\n",
    "        print(f\"  ✓ Saved to {OUTPUT_DIR}/generate_directions/mean_diffs.pt\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Direction generation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Direction Selection\n",
    "\n",
    "**Epistemic Status: LOW-MODERATE (40-60%)**  \n",
    "Highly uncertain what \"self-other tokens\" should be.\n",
    "\n",
    "**Challenge**: Unlike refusal (clear tokens: \"I cannot\", \"I apologize\") or ToM (question answers), self-other distinction is about pronouns and sentence subjects, which are deeply embedded in grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokens for self-reference\n",
    "# These are first-person pronouns that indicate self-reference\n",
    "SELF_TOKENS = model_base.tokenizer.encode(\n",
    "    \" I my me myself mine\",\n",
    "    add_special_tokens=False\n",
    ")\n",
    "\n",
    "# Define tokens for other-reference\n",
    "OTHER_TOKENS = model_base.tokenizer.encode(\n",
    "    \" he she his her they them their the model assistant\",\n",
    "    add_special_tokens=False\n",
    ")\n",
    "\n",
    "print(f\"Self tokens: {SELF_TOKENS}\")\n",
    "print(f\"Decoded: {[model_base.tokenizer.decode([t]) for t in SELF_TOKENS]}\")\n",
    "print(f\"\\nOther tokens: {OTHER_TOKENS}\")\n",
    "print(f\"Decoded: {[model_base.tokenizer.decode([t]) for t in OTHER_TOKENS]}\")\n",
    "print(f\"\\n⚠️ NOTE: This is a highly uncertain proxy.\")\n",
    "print(f\"Self-other distinction is grammatical, not behavioral.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_direction_self_other(\n",
    "    model_base,\n",
    "    pos: int,\n",
    "    layer: int, \n",
    "    direction: torch.Tensor,\n",
    "    self_val: List[str],\n",
    "    other_val: List[str],\n",
    "    self_tokens: List[int]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a direction for self-other ablation.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with bypass_score, induce_score, kl_score\n",
    "    \"\"\"\n",
    "    # Normalize\n",
    "    direction_normalized = direction / direction.norm()\n",
    "    \n",
    "    # Get hooks\n",
    "    ablation_pre_hooks, ablation_hooks = get_all_direction_ablation_hooks(\n",
    "        model_base, direction_normalized\n",
    "    )\n",
    "    \n",
    "    actadd_pre_hooks = [(\n",
    "        model_base.model_block_modules[layer],\n",
    "        get_activation_addition_input_pre_hook(vector=direction_normalized, coeff=-1.0)\n",
    "    )]\n",
    "    actadd_hooks = []\n",
    "    \n",
    "    # Baseline scores on self (high self-token probability)\n",
    "    baseline_self_scores = get_refusal_scores(\n",
    "        model_base.model, self_val, model_base.tokenize_instructions_fn,\n",
    "        self_tokens, fwd_pre_hooks=[], fwd_hooks=[]\n",
    "    )\n",
    "    \n",
    "    # Ablation scores on self (should be lower - less self-reference)\n",
    "    ablation_self_scores = get_refusal_scores(\n",
    "        model_base.model, self_val, model_base.tokenize_instructions_fn,\n",
    "        self_tokens, fwd_pre_hooks=ablation_pre_hooks, fwd_hooks=ablation_hooks\n",
    "    )\n",
    "    \n",
    "    # Baseline scores on other (low self-token probability)\n",
    "    baseline_other_scores = get_refusal_scores(\n",
    "        model_base.model, other_val, model_base.tokenize_instructions_fn,\n",
    "        self_tokens, fwd_pre_hooks=[], fwd_hooks=[]\n",
    "    )\n",
    "    \n",
    "    # Actadd scores on other (should be higher - more self-reference)\n",
    "    actadd_other_scores = get_refusal_scores(\n",
    "        model_base.model, other_val, model_base.tokenize_instructions_fn,\n",
    "        self_tokens, fwd_pre_hooks=actadd_pre_hooks, fwd_hooks=actadd_hooks\n",
    "    )\n",
    "    \n",
    "    # KL divergence on other examples\n",
    "    baseline_logits = get_last_position_logits(\n",
    "        model_base.model, model_base.tokenizer, other_val,\n",
    "        model_base.tokenize_instructions_fn, fwd_pre_hooks=[], fwd_hooks=[]\n",
    "    )\n",
    "    \n",
    "    ablation_logits = get_last_position_logits(\n",
    "        model_base.model, model_base.tokenizer, other_val,\n",
    "        model_base.tokenize_instructions_fn, fwd_pre_hooks=ablation_pre_hooks, fwd_hooks=ablation_hooks\n",
    "    )\n",
    "    \n",
    "    baseline_probs = torch.nn.functional.softmax(baseline_logits, dim=-1)\n",
    "    ablation_probs = torch.nn.functional.softmax(ablation_logits, dim=-1)\n",
    "    kl_div = torch.nn.functional.kl_div(\n",
    "        ablation_probs.log(), baseline_probs, reduction='batchmean'\n",
    "    )\n",
    "    \n",
    "    # Compute scores\n",
    "    bypass_score = (baseline_self_scores.mean() - ablation_self_scores.mean()).item()\n",
    "    induce_score = (actadd_other_scores.mean() - baseline_other_scores.mean()).item()\n",
    "    kl_score = kl_div.item()\n",
    "    \n",
    "    return {\n",
    "        'bypass_score': bypass_score,\n",
    "        'induce_score': induce_score,\n",
    "        'kl_score': kl_score,\n",
    "    }\n",
    "\n",
    "print(\"✓ Direction evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Evaluating candidate directions...\")\n",
    "    print(f\"Estimated time: ~20-40 minutes\\n\")\n",
    "    \n",
    "    n_positions = candidate_directions.shape[0]\n",
    "    n_layers = candidate_directions.shape[1]\n",
    "    max_layer = int(n_layers * 0.8)\n",
    "    \n",
    "    evaluations = []\n",
    "    \n",
    "    for pos in tqdm(range(n_positions), desc=\"Positions\"):\n",
    "        for layer in tqdm(range(max_layer), desc=\"Layers\", leave=False):\n",
    "            direction = candidate_directions[pos, layer, :]\n",
    "            \n",
    "            eval_result = evaluate_direction_self_other(\n",
    "                model_base, pos, layer, direction,\n",
    "                self_val, other_val, SELF_TOKENS\n",
    "            )\n",
    "            \n",
    "            evaluations.append({\n",
    "                'pos': pos - n_positions,\n",
    "                'layer': layer,\n",
    "                **eval_result\n",
    "            })\n",
    "    \n",
    "    # Filter\n",
    "    filtered_evaluations = [\n",
    "        e for e in evaluations\n",
    "        if e['kl_score'] < KL_THRESHOLD and e['induce_score'] > INDUCE_THRESHOLD\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n✓ Evaluated {len(evaluations)} candidates\")\n",
    "    print(f\"  Filtered to {len(filtered_evaluations)} candidates\")\n",
    "    \n",
    "    if len(filtered_evaluations) > 0:\n",
    "        best = max(filtered_evaluations, key=lambda x: x['bypass_score'])\n",
    "        \n",
    "        best_pos = best['pos']\n",
    "        best_layer = best['layer']\n",
    "        best_direction = candidate_directions[best_pos + n_positions, best_layer, :]\n",
    "        \n",
    "        print(f\"\\n✓ Best direction selected\")\n",
    "        print(f\"  Position: {best_pos}\")\n",
    "        print(f\"  Layer: {best_layer}\")\n",
    "        print(f\"  Bypass score: {best['bypass_score']:.4f}\")\n",
    "        print(f\"  Induce score: {best['induce_score']:.4f}\")\n",
    "        print(f\"  KL score: {best['kl_score']:.4f}\")\n",
    "        \n",
    "        # Save\n",
    "        if SAVE_ARTIFACTS:\n",
    "            os.makedirs(f\"{OUTPUT_DIR}/select_direction\", exist_ok=True)\n",
    "            \n",
    "            with open(f\"{OUTPUT_DIR}/select_direction/direction_evaluations.json\", 'w') as f:\n",
    "                json.dump(evaluations, f, indent=2)\n",
    "            \n",
    "            with open(f\"{OUTPUT_DIR}/select_direction/direction_evaluations_filtered.json\", 'w') as f:\n",
    "                json.dump(filtered_evaluations, f, indent=2)\n",
    "            \n",
    "            torch.save(best_direction, f\"{OUTPUT_DIR}/direction.pt\")\n",
    "            \n",
    "            with open(f\"{OUTPUT_DIR}/direction_metadata.json\", 'w') as f:\n",
    "                json.dump({'pos': best_pos, 'layer': best_layer}, f, indent=2)\n",
    "            \n",
    "            print(f\"  ✓ Saved to {OUTPUT_DIR}\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ No directions passed filtering!\")\n",
    "        print(f\"Try relaxing thresholds: KL_THRESHOLD={KL_THRESHOLD}, INDUCE_THRESHOLD={INDUCE_THRESHOLD}\")\n",
    "        best_direction = None\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Direction selection failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Intervention and Evaluation\n",
    "\n",
    "**Epistemic Status: LOW (30-40%)**  \n",
    "Very uncertain if this will work meaningfully.\n",
    "\n",
    "Expected behavior if successful:\n",
    "- **Ablation**: Model should avoid first-person pronouns, use third-person instead\n",
    "- **Activation addition**: Model should use more first-person language\n",
    "\n",
    "Likely outcome: Either no effect, or grammatical breakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_direction is not None:\n",
    "    print(\"Setting up interventions...\")\n",
    "    \n",
    "    direction_normalized = best_direction / best_direction.norm()\n",
    "    \n",
    "    baseline_pre_hooks, baseline_hooks = [], []\n",
    "    ablation_pre_hooks, ablation_hooks = get_all_direction_ablation_hooks(\n",
    "        model_base, direction_normalized\n",
    "    )\n",
    "    actadd_pre_hooks = [(\n",
    "        model_base.model_block_modules[best_layer],\n",
    "        get_activation_addition_input_pre_hook(vector=direction_normalized, coeff=ACTIVATION_ADD_COEFF)\n",
    "    )]\n",
    "    actadd_hooks = []\n",
    "    \n",
    "    print(\"✓ Interventions configured\")\n",
    "else:\n",
    "    print(\"⚠️ Skipping - no direction selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_direction is not None:\n",
    "    print(\"Testing interventions on examples...\\n\")\n",
    "    \n",
    "    n_examples = min(5, len(test_data))\n",
    "    \n",
    "    for i in range(n_examples):\n",
    "        item = test_data[i]\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(f\"Example {i+1}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Test with self prompt\n",
    "        print(f\"\\n[Input: SELF version]\")\n",
    "        print(f\"{item['self_subject'][:200]}...\" if len(item['self_subject']) > 200 else item['self_subject'])\n",
    "        \n",
    "        self_prompts = [item['self_subject']]\n",
    "        \n",
    "        # Generate with baseline\n",
    "        baseline_completions = model_base.generate_completions(\n",
    "            self_prompts,\n",
    "            fwd_pre_hooks=baseline_pre_hooks,\n",
    "            fwd_hooks=baseline_hooks,\n",
    "            max_new_tokens=MAX_NEW_TOKENS\n",
    "        )\n",
    "        print(f\"\\nBaseline: {baseline_completions[0][:300]}...\" if len(baseline_completions[0]) > 300 else f\"\\nBaseline: {baseline_completions[0]}\")\n",
    "        \n",
    "        # Generate with ablation\n",
    "        ablation_completions = model_base.generate_completions(\n",
    "            self_prompts,\n",
    "            fwd_pre_hooks=ablation_pre_hooks,\n",
    "            fwd_hooks=ablation_hooks,\n",
    "            max_new_tokens=MAX_NEW_TOKENS\n",
    "        )\n",
    "        print(f\"\\nAblation (self removed): {ablation_completions[0][:300]}...\" if len(ablation_completions[0]) > 300 else f\"\\nAblation: {ablation_completions[0]}\")\n",
    "        \n",
    "        # Count first-person pronouns\n",
    "        baseline_first_person = sum(p in baseline_completions[0].lower() for p in ['i ', 'my ', 'me ', 'myself'])\n",
    "        ablation_first_person = sum(p in ablation_completions[0].lower() for p in ['i ', 'my ', 'me ', 'myself'])\n",
    "        \n",
    "        print(f\"\\nFirst-person pronouns: baseline={baseline_first_person}, ablation={ablation_first_person}\")\n",
    "        print()\n",
    "        \n",
    "    print(\"\\n⚠️ Manual evaluation:\")\n",
    "    print(\"  - Does ablation reduce first-person language?\")\n",
    "    print(\"  - Does the model switch to third-person?\")\n",
    "    print(\"  - Or does it just break grammatically?\")\n",
    "    print(\"  - Is the output still coherent?\")\n",
    "else:\n",
    "    print(\"⚠️ Skipping - no direction selected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Reflections\n",
    "\n",
    "### What We Did\n",
    "1. ✅ Loaded self-other contrast dataset (400 examples)\n",
    "2. ✅ Generated directions via difference-in-means\n",
    "3. ✅ Selected direction using bypass/induce/KL scores\n",
    "4. ✅ Applied interventions and evaluated\n",
    "\n",
    "### Key Challenges\n",
    "1. **Small dataset**: Only 400 examples may be insufficient\n",
    "2. **Unclear metric**: What tokens indicate \"self-other distinction\"?\n",
    "3. **Grammatical feature**: Self-reference is grammatical, not cognitive\n",
    "4. **Entanglement**: First-person usage is fundamental to language\n",
    "\n",
    "### Expected Results\n",
    "Most likely outcomes:\n",
    "1. **No effect**: Direction doesn't capture self-other in a meaningful way\n",
    "2. **Grammatical breakage**: Ablation damages grammar without changing perspective\n",
    "3. **Superficial change**: Changes pronouns but not reasoning\n",
    "\n",
    "### Why This Dataset is Less Suitable\n",
    "Unlike refusal (behavioral) or ToM (cognitive), self-other distinction as tested here is primarily:\n",
    "- **Linguistic**: Pronoun choice\n",
    "- **Context-dependent**: Depends on who is speaking\n",
    "- **Fundamental**: Too basic to isolate without damage\n",
    "\n",
    "### Better Approaches\n",
    "1. **Instruction tuning**: Fine-tune model to use third-person\n",
    "2. **System prompts**: Add \"Refer to yourself in third person\"\n",
    "3. **Post-processing**: Replace pronouns after generation\n",
    "4. **Steering vectors**: More flexible than ablation\n",
    "\n",
    "### If You Still Want to Try This\n",
    "Consider testing on:\n",
    "- Self-awareness questions (\"Do you have experiences?\")\n",
    "- Perspective-taking tasks (\"What do I know vs what do you know?\")\n",
    "- Theory of Mind about self vs others\n",
    "\n",
    "### Files Saved\n",
    "- `{OUTPUT_DIR}/generate_directions/mean_diffs.pt`\n",
    "- `{OUTPUT_DIR}/direction.pt`\n",
    "- `{OUTPUT_DIR}/direction_metadata.json`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
