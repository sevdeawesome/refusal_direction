{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory of Mind Directional Ablation - SimpleTOM Dataset\n",
    "\n",
    "## Overview\n",
    "This notebook implements **orthogonal ablation** to remove Theory-of-Mind (ToM) capabilities from LLMs, based on the method from \"Refusal in LLMs is Mediated by a Single Direction\" (Arditi et al., 2024).\n",
    "\n",
    "## Epistemic Status: Dataset Suitability\n",
    "\n",
    "**Confidence: MODERATE (60-70%)**\n",
    "\n",
    "### Why this dataset might work:\n",
    "- ✅ **Clear contrast**: `high_tom_prompt` requires mental state reasoning (\"Is Mary aware that...\") vs `low_tom_prompt` asks factual questions (\"Is the following statement true...\").\n",
    "- ✅ **Same scenario**: Both prompts share the same underlying scenario, isolating ToM reasoning from factual knowledge.\n",
    "- ✅ **Large dataset**: 3,441 examples provide sufficient training data.\n",
    "- ✅ **Diverse scenarios**: Multiple question types (awareness, beliefs) and scenario types.\n",
    "\n",
    "### Potential concerns:\n",
    "- ⚠️ **Capability vs behavior**: Directional ablation was designed for refusal (a behavioral pattern). ToM is a cognitive capability - the method may be less effective.\n",
    "- ⚠️ **Entanglement**: ToM might be distributed across many directions rather than concentrated in a single direction.\n",
    "- ⚠️ **Transfer**: The selected direction may not generalize to other ToM tasks beyond this specific format.\n",
    "- ⚠️ **Side effects**: Removing ToM might damage other reasoning capabilities due to feature entanglement.\n",
    "\n",
    "### Alternative methods to consider:\n",
    "- **Orthogonal steering vectors** (more general than single-direction ablation)\n",
    "- **Activation patching** to identify which components are causally important\n",
    "- **Sparse probing** to find multiple directions\n",
    "\n",
    "## Method Summary\n",
    "\n",
    "1. **Generate directions**: Compute `r = mean(high_tom_activations) - mean(low_tom_activations)` for each layer/position\n",
    "2. **Select direction**: Evaluate candidates using:\n",
    "   - `bypass_score`: How well ablation removes ToM\n",
    "   - `induce_score`: How well activation addition adds ToM\n",
    "   - `kl_score`: Distribution shift on neutral examples (lower is better)\n",
    "3. **Apply intervention**:\n",
    "   - **Ablation**: Remove direction: `x' = x - r̂(r̂ᵀx)`\n",
    "   - **Activation addition**: Add direction: `x' = x + αr`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
    "\n",
    "# Scientific computing\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Project imports\n",
    "from pipeline.config import Config\n",
    "from pipeline.model_utils.model_factory import construct_model_base\n",
    "from pipeline.utils.hook_utils import (\n",
    "    get_activation_addition_input_pre_hook,\n",
    "    get_all_direction_ablation_hooks\n",
    ")\n",
    "from pipeline.submodules.generate_directions import get_mean_diff, get_mean_activations\n",
    "from pipeline.submodules.select_direction import get_refusal_scores, get_last_position_logits\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Variables\n",
    "\n",
    "**⚠️ IMPORTANT: All major configuration parameters are defined here in CAPS**\n",
    "\n",
    "### Model Configuration\n",
    "- `MODEL_PATH`: HuggingFace model path or local path\n",
    "- `DEVICE`: Device to run on (cuda/cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION PARAMETERS - MODIFY THESE\n",
    "# ============================================================================\n",
    "\n",
    "# Model settings\n",
    "MODEL_PATH = \"meta-llama/Llama-3.2-1B-Instruct\"  # Small Llama model for testing\n",
    "# MODEL_PATH = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # Larger model\n",
    "# MODEL_PATH = \"Qwen/Qwen2.5-32B-Instruct\"  # Even larger model\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Dataset settings\n",
    "DATASET_PATH = \"tom_dataset/simpletom_contrast_pairs.json\"\n",
    "N_TRAIN = 128  # Number of training examples for direction generation\n",
    "N_VAL = 32     # Number of validation examples for direction selection\n",
    "N_TEST = 50    # Number of test examples for evaluation\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Direction generation settings\n",
    "BATCH_SIZE = 32  # Batch size for activation collection\n",
    "GENERATION_BATCH_SIZE = 8  # Batch size for text generation\n",
    "\n",
    "# Direction selection settings  \n",
    "KL_THRESHOLD = 0.1  # Maximum KL divergence allowed\n",
    "INDUCE_THRESHOLD = 0.0  # Minimum induction score\n",
    "\n",
    "# Intervention settings\n",
    "ABLATION_COEFF = 1.0  # Coefficient for ablation\n",
    "ACTIVATION_ADD_COEFF = -1.0  # Coefficient for activation addition (negative removes ToM)\n",
    "MAX_NEW_TOKENS = 512  # Maximum tokens to generate\n",
    "\n",
    "# Output settings\n",
    "OUTPUT_DIR = \"pipeline/runs/simpletom_experiment\"\n",
    "SAVE_ARTIFACTS = True  # Whether to save intermediate results\n",
    "\n",
    "print(f\"✓ Configuration set\")\n",
    "print(f\"  Model: {MODEL_PATH}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Train/Val/Test: {N_TRAIN}/{N_VAL}/{N_TEST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading and Inspection\n",
    "\n",
    "**Epistemic Status: HIGH (90%+)**  \n",
    "Dataset loading and structure is straightforward and can be validated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_simpletom_dataset(dataset_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load SimpleTOM contrast pairs dataset.\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with keys: high_tom_prompt, low_tom_prompt, etc.\n",
    "    \"\"\"\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_simpletom_dataset(DATASET_PATH)\n",
    "print(f\"✓ Loaded {len(dataset)} contrast pairs\")\n",
    "\n",
    "# Display a sample\n",
    "sample = dataset[0]\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE CONTRAST PAIR\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nScenario: {sample['scenario']}\")\n",
    "print(f\"\\n[HIGH ToM - Requires mental state reasoning]\")\n",
    "print(f\"{sample['high_tom_prompt']}\")\n",
    "print(f\"Expected: {sample['high_tom_completion']}\")\n",
    "print(f\"\\n[LOW ToM - Factual question]\")\n",
    "print(f\"{sample['low_tom_prompt']}\")\n",
    "print(f\"Expected: {sample['low_tom_completion']}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Split and Validation\n",
    "\n",
    "**Epistemic Status: HIGH (90%+)**  \n",
    "Simple random sampling with validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset: List[Dict], n_train: int, n_val: int, n_test: int, \n",
    "                  seed: int = 42) -> Tuple[List[str], List[str], List[str], List[str], List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Split dataset into train/val/test sets for high_tom and low_tom.\n",
    "    \n",
    "    Returns:\n",
    "        (high_tom_train, low_tom_train, high_tom_val, low_tom_val, test_dataset, full_test_items)\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    shuffled = random.sample(dataset, len(dataset))\n",
    "    \n",
    "    # Split\n",
    "    train_data = shuffled[:n_train]\n",
    "    val_data = shuffled[n_train:n_train+n_val]\n",
    "    test_data = shuffled[n_train+n_val:n_train+n_val+n_test]\n",
    "    \n",
    "    # Extract high_tom and low_tom prompts\n",
    "    high_tom_train = [item['high_tom_prompt'] for item in train_data]\n",
    "    low_tom_train = [item['low_tom_prompt'] for item in train_data]\n",
    "    \n",
    "    high_tom_val = [item['high_tom_prompt'] for item in val_data]\n",
    "    low_tom_val = [item['low_tom_prompt'] for item in val_data]\n",
    "    \n",
    "    return high_tom_train, low_tom_train, high_tom_val, low_tom_val, test_data, test_data\n",
    "\n",
    "# Split dataset\n",
    "high_tom_train, low_tom_train, high_tom_val, low_tom_val, test_data, full_test_items = split_dataset(\n",
    "    dataset, N_TRAIN, N_VAL, N_TEST, RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"✓ Dataset split complete\")\n",
    "print(f\"  Train: {len(high_tom_train)} high_tom, {len(low_tom_train)} low_tom\")\n",
    "print(f\"  Val: {len(high_tom_val)} high_tom, {len(low_tom_val)} low_tom\")\n",
    "print(f\"  Test: {len(test_data)} pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Unit Tests for Dataset\n",
    "\n",
    "**Epistemic Status: HIGH (95%+)**  \n",
    "These tests verify dataset integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_structure():\n",
    "    \"\"\"Test that dataset has expected structure.\"\"\"\n",
    "    print(\"Running dataset structure tests...\")\n",
    "    \n",
    "    # Test 1: All items have required keys\n",
    "    required_keys = ['high_tom_prompt', 'low_tom_prompt', 'high_tom_completion', \n",
    "                     'low_tom_completion', 'scenario', 'category']\n",
    "    \n",
    "    for i, item in enumerate(dataset[:100]):\n",
    "        for key in required_keys:\n",
    "            assert key in item, f\"Item {i} missing key: {key}\"\n",
    "    print(\"  ✓ All items have required keys\")\n",
    "    \n",
    "    # Test 2: Prompts share same scenario\n",
    "    for i, item in enumerate(dataset[:100]):\n",
    "        scenario = item['scenario']\n",
    "        assert scenario in item['high_tom_prompt'], f\"Item {i}: scenario not in high_tom_prompt\"\n",
    "        assert scenario in item['low_tom_prompt'], f\"Item {i}: scenario not in low_tom_prompt\"\n",
    "    print(\"  ✓ Prompts share same scenario\")\n",
    "    \n",
    "    # Test 3: High ToM prompts ask about mental states\n",
    "    mental_state_keywords = ['aware', 'know', 'believe', 'think', 'likely']\n",
    "    count = 0\n",
    "    for item in dataset[:100]:\n",
    "        if any(keyword in item['high_tom_prompt'].lower() for keyword in mental_state_keywords):\n",
    "            count += 1\n",
    "    assert count > 50, f\"Only {count}/100 high_tom prompts contain mental state keywords\"\n",
    "    print(f\"  ✓ {count}/100 high_tom prompts contain mental state keywords\")\n",
    "    \n",
    "    # Test 4: Low ToM prompts are factual\n",
    "    factual_keywords = ['true', 'correct', 'fact']\n",
    "    count = 0\n",
    "    for item in dataset[:100]:\n",
    "        if any(keyword in item['low_tom_prompt'].lower() for keyword in factual_keywords):\n",
    "            count += 1\n",
    "    assert count > 50, f\"Only {count}/100 low_tom prompts contain factual keywords\"\n",
    "    print(f\"  ✓ {count}/100 low_tom prompts contain factual keywords\")\n",
    "    \n",
    "    # Test 5: No data leakage between splits\n",
    "    train_ids = {item['id'] for item in dataset[:N_TRAIN]}\n",
    "    val_ids = {item['id'] for item in dataset[N_TRAIN:N_TRAIN+N_VAL]}\n",
    "    test_ids = {item['id'] for item in dataset[N_TRAIN+N_VAL:N_TRAIN+N_VAL+N_TEST]}\n",
    "    \n",
    "    assert len(train_ids & val_ids) == 0, \"Data leakage between train and val\"\n",
    "    assert len(train_ids & test_ids) == 0, \"Data leakage between train and test\"\n",
    "    assert len(val_ids & test_ids) == 0, \"Data leakage between val and test\"\n",
    "    print(\"  ✓ No data leakage between splits\")\n",
    "    \n",
    "    print(\"\\n✓ All dataset tests passed!\")\n",
    "\n",
    "# Run tests\n",
    "test_dataset_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Loading\n",
    "\n",
    "**Epistemic Status: MODERATE-HIGH (70-80%)**  \n",
    "Model loading depends on HuggingFace availability and GPU resources. Without GPU access, this cell will fail or be very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Construct model using factory\n",
    "    print(f\"Loading model: {MODEL_PATH}\")\n",
    "    print(f\"This may take several minutes...\")\n",
    "    \n",
    "    model_base = construct_model_base(MODEL_PATH)\n",
    "    \n",
    "    print(f\"\\n✓ Model loaded successfully\")\n",
    "    print(f\"  Model: {model_base.model.__class__.__name__}\")\n",
    "    print(f\"  Tokenizer: {model_base.tokenizer.__class__.__name__}\")\n",
    "    print(f\"  Device: {model_base.model.device}\")\n",
    "    print(f\"  Num layers: {model_base.model.config.num_hidden_layers}\")\n",
    "    print(f\"  Hidden size: {model_base.model.config.hidden_size}\")\n",
    "    print(f\"  EOI tokens: {model_base.eoi_toks}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Model loading failed: {e}\")\n",
    "    print(f\"\\nThis is expected if:\")\n",
    "    print(f\"  - You don't have GPU access\")\n",
    "    print(f\"  - The model is not available on HuggingFace\")\n",
    "    print(f\"  - You don't have HuggingFace authentication set up\")\n",
    "    print(f\"\\nYou can still review the code structure without running it.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Direction Generation\n",
    "\n",
    "**Epistemic Status: HIGH (85%+)**  \n",
    "This is the core method from the paper. The implementation is sound.\n",
    "\n",
    "We compute the difference-in-means between high_tom and low_tom activations:\n",
    "```\n",
    "r[pos, layer] = mean(high_tom_activations) - mean(low_tom_activations)\n",
    "```\n",
    "\n",
    "**⚠️ Note**: This requires GPU and can take 10-30 minutes depending on model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Generating candidate directions...\")\n",
    "    print(f\"This will process {N_TRAIN} examples in batches of {BATCH_SIZE}\")\n",
    "    print(f\"Estimated time: ~10-30 minutes depending on model size\\n\")\n",
    "    \n",
    "    # Generate directions using the difference-in-means method\n",
    "    # This computes: mean(high_tom_activations) - mean(low_tom_activations)\n",
    "    # for each position in eoi_toks and each layer\n",
    "    \n",
    "    candidate_directions = get_mean_diff(\n",
    "        model=model_base.model,\n",
    "        tokenizer=model_base.tokenizer,\n",
    "        harmful_instructions=high_tom_train,  # Use high_tom as \"harmful\" (concept to remove)\n",
    "        harmless_instructions=low_tom_train,   # Use low_tom as \"harmless\" (baseline)\n",
    "        tokenize_instructions_fn=model_base.tokenize_instructions_fn,\n",
    "        block_modules=model_base.model_block_modules,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        positions=list(range(-len(model_base.eoi_toks), 0))  # Extract from end-of-instruction positions\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Generated candidate directions\")\n",
    "    print(f\"  Shape: {candidate_directions.shape}\")\n",
    "    print(f\"  Expected: (n_positions={len(model_base.eoi_toks)}, n_layers={model_base.model.config.num_hidden_layers}, d_model={model_base.model.config.hidden_size})\")\n",
    "    print(f\"  Dtype: {candidate_directions.dtype}\")\n",
    "    print(f\"  Device: {candidate_directions.device}\")\n",
    "    \n",
    "    # Validate\n",
    "    assert not candidate_directions.isnan().any(), \"NaN values in candidate directions!\"\n",
    "    assert candidate_directions.shape[0] == len(model_base.eoi_toks)\n",
    "    assert candidate_directions.shape[1] == model_base.model.config.num_hidden_layers\n",
    "    print(\"  ✓ Validation passed\")\n",
    "    \n",
    "    # Save if requested\n",
    "    if SAVE_ARTIFACTS:\n",
    "        os.makedirs(f\"{OUTPUT_DIR}/generate_directions\", exist_ok=True)\n",
    "        torch.save(candidate_directions, f\"{OUTPUT_DIR}/generate_directions/mean_diffs.pt\")\n",
    "        print(f\"  ✓ Saved to {OUTPUT_DIR}/generate_directions/mean_diffs.pt\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Direction generation failed: {e}\")\n",
    "    print(f\"This is expected without GPU access.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Direction Selection\n",
    "\n",
    "**Epistemic Status: MODERATE (60-70%)**  \n",
    "This is where things get uncertain for ToM ablation.\n",
    "\n",
    "We evaluate each candidate direction on three metrics:\n",
    "1. **bypass_score**: How much does ablating this direction reduce ToM capability?\n",
    "2. **induce_score**: How much does adding this direction increase ToM capability?\n",
    "3. **kl_score**: How much does this intervention shift the output distribution?\n",
    "\n",
    "**⚠️ Key Uncertainty**: The original paper used \"refusal tokens\" to measure refusal behavior. For ToM, we need to define what \"ToM tokens\" are. This is non-trivial!\n",
    "\n",
    "**Approach**: We'll use the model's probability of generating the correct answer on ToM questions as a proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define \"ToM tokens\" - tokens that indicate ToM reasoning\n",
    "# This is a heuristic and may need tuning!\n",
    "TOM_TOKENS = model_base.tokenizer.encode(\n",
    "    \" Yes No aware know believe think likely\",\n",
    "    add_special_tokens=False\n",
    ")\n",
    "\n",
    "print(f\"Using ToM tokens: {TOM_TOKENS}\")\n",
    "print(f\"Decoded: {[model_base.tokenizer.decode([t]) for t in TOM_TOKENS]}\")\n",
    "print(f\"\\n⚠️ NOTE: This is a heuristic proxy for ToM capability.\")\n",
    "print(f\"The original paper used refusal tokens like 'I cannot', 'I apologize'.\")\n",
    "print(f\"For ToM, there's no clear equivalent, so we use answer tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_direction(\n",
    "    model_base,\n",
    "    pos: int,\n",
    "    layer: int, \n",
    "    direction: torch.Tensor,\n",
    "    high_tom_val: List[str],\n",
    "    low_tom_val: List[str],\n",
    "    tom_tokens: List[int]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a single candidate direction.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with bypass_score, induce_score, kl_score\n",
    "    \"\"\"\n",
    "    # Normalize direction\n",
    "    direction_normalized = direction / direction.norm()\n",
    "    \n",
    "    # Get hooks for ablation\n",
    "    ablation_pre_hooks, ablation_hooks = get_all_direction_ablation_hooks(\n",
    "        model_base, direction_normalized\n",
    "    )\n",
    "    \n",
    "    # Get hooks for activation addition\n",
    "    actadd_pre_hooks = [(\n",
    "        model_base.model_block_modules[layer],\n",
    "        get_activation_addition_input_pre_hook(vector=direction_normalized, coeff=-1.0)\n",
    "    )]\n",
    "    actadd_hooks = []\n",
    "    \n",
    "    # Baseline scores on high_tom (should be high before intervention)\n",
    "    baseline_high_tom_scores = get_refusal_scores(\n",
    "        model_base.model, high_tom_val, model_base.tokenize_instructions_fn,\n",
    "        tom_tokens, fwd_pre_hooks=[], fwd_hooks=[]\n",
    "    )\n",
    "    \n",
    "    # Ablation scores on high_tom (should be lower after ablation)\n",
    "    ablation_high_tom_scores = get_refusal_scores(\n",
    "        model_base.model, high_tom_val, model_base.tokenize_instructions_fn,\n",
    "        tom_tokens, fwd_pre_hooks=ablation_pre_hooks, fwd_hooks=ablation_hooks\n",
    "    )\n",
    "    \n",
    "    # Activation addition scores on low_tom (should be higher after adding direction)\n",
    "    baseline_low_tom_scores = get_refusal_scores(\n",
    "        model_base.model, low_tom_val, model_base.tokenize_instructions_fn,\n",
    "        tom_tokens, fwd_pre_hooks=[], fwd_hooks=[]\n",
    "    )\n",
    "    \n",
    "    actadd_low_tom_scores = get_refusal_scores(\n",
    "        model_base.model, low_tom_val, model_base.tokenize_instructions_fn,\n",
    "        tom_tokens, fwd_pre_hooks=actadd_pre_hooks, fwd_hooks=actadd_hooks\n",
    "    )\n",
    "    \n",
    "    # Compute KL divergence on neutral examples (using low_tom as neutral)\n",
    "    baseline_logits = get_last_position_logits(\n",
    "        model_base.model, model_base.tokenizer, low_tom_val,\n",
    "        model_base.tokenize_instructions_fn, fwd_pre_hooks=[], fwd_hooks=[]\n",
    "    )\n",
    "    \n",
    "    ablation_logits = get_last_position_logits(\n",
    "        model_base.model, model_base.tokenizer, low_tom_val,\n",
    "        model_base.tokenize_instructions_fn, fwd_pre_hooks=ablation_pre_hooks, fwd_hooks=ablation_hooks\n",
    "    )\n",
    "    \n",
    "    # KL divergence\n",
    "    baseline_probs = torch.nn.functional.softmax(baseline_logits, dim=-1)\n",
    "    ablation_probs = torch.nn.functional.softmax(ablation_logits, dim=-1)\n",
    "    kl_div = torch.nn.functional.kl_div(\n",
    "        ablation_probs.log(), baseline_probs, reduction='batchmean'\n",
    "    )\n",
    "    \n",
    "    # Compute scores\n",
    "    bypass_score = (baseline_high_tom_scores.mean() - ablation_high_tom_scores.mean()).item()\n",
    "    induce_score = (actadd_low_tom_scores.mean() - baseline_low_tom_scores.mean()).item()\n",
    "    kl_score = kl_div.item()\n",
    "    \n",
    "    return {\n",
    "        'bypass_score': bypass_score,\n",
    "        'induce_score': induce_score,\n",
    "        'kl_score': kl_score,\n",
    "    }\n",
    "\n",
    "print(\"✓ Direction evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Best Direction\n",
    "\n",
    "Now we evaluate all candidate directions and select the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Evaluating candidate directions...\")\n",
    "    print(f\"This will evaluate {candidate_directions.shape[0] * candidate_directions.shape[1]} candidates\")\n",
    "    print(f\"Estimated time: ~30-60 minutes\\n\")\n",
    "    \n",
    "    n_positions = candidate_directions.shape[0]\n",
    "    n_layers = candidate_directions.shape[1]\n",
    "    \n",
    "    # Filter out top 20% of layers (too close to output)\n",
    "    max_layer = int(n_layers * 0.8)\n",
    "    \n",
    "    evaluations = []\n",
    "    \n",
    "    for pos in tqdm(range(n_positions), desc=\"Positions\"):\n",
    "        for layer in tqdm(range(max_layer), desc=\"Layers\", leave=False):\n",
    "            direction = candidate_directions[pos, layer, :]\n",
    "            \n",
    "            eval_result = evaluate_direction(\n",
    "                model_base, pos, layer, direction,\n",
    "                high_tom_val, low_tom_val, TOM_TOKENS\n",
    "            )\n",
    "            \n",
    "            evaluations.append({\n",
    "                'pos': pos - n_positions,  # Convert to negative index\n",
    "                'layer': layer,\n",
    "                **eval_result\n",
    "            })\n",
    "    \n",
    "    # Filter by thresholds\n",
    "    filtered_evaluations = [\n",
    "        e for e in evaluations\n",
    "        if e['kl_score'] < KL_THRESHOLD and e['induce_score'] > INDUCE_THRESHOLD\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n✓ Evaluated {len(evaluations)} candidates\")\n",
    "    print(f\"  Filtered to {len(filtered_evaluations)} candidates\")\n",
    "    \n",
    "    # Select best by bypass_score\n",
    "    if len(filtered_evaluations) > 0:\n",
    "        best = max(filtered_evaluations, key=lambda x: x['bypass_score'])\n",
    "        \n",
    "        best_pos = best['pos']\n",
    "        best_layer = best['layer']\n",
    "        best_direction = candidate_directions[best_pos + n_positions, best_layer, :]\n",
    "        \n",
    "        print(f\"\\n✓ Best direction selected\")\n",
    "        print(f\"  Position: {best_pos}\")\n",
    "        print(f\"  Layer: {best_layer}\")\n",
    "        print(f\"  Bypass score: {best['bypass_score']:.4f}\")\n",
    "        print(f\"  Induce score: {best['induce_score']:.4f}\")\n",
    "        print(f\"  KL score: {best['kl_score']:.4f}\")\n",
    "        \n",
    "        # Save\n",
    "        if SAVE_ARTIFACTS:\n",
    "            os.makedirs(f\"{OUTPUT_DIR}/select_direction\", exist_ok=True)\n",
    "            \n",
    "            with open(f\"{OUTPUT_DIR}/select_direction/direction_evaluations.json\", 'w') as f:\n",
    "                json.dump(evaluations, f, indent=2)\n",
    "            \n",
    "            with open(f\"{OUTPUT_DIR}/select_direction/direction_evaluations_filtered.json\", 'w') as f:\n",
    "                json.dump(filtered_evaluations, f, indent=2)\n",
    "            \n",
    "            torch.save(best_direction, f\"{OUTPUT_DIR}/direction.pt\")\n",
    "            \n",
    "            with open(f\"{OUTPUT_DIR}/direction_metadata.json\", 'w') as f:\n",
    "                json.dump({'pos': best_pos, 'layer': best_layer}, f, indent=2)\n",
    "            \n",
    "            print(f\"  ✓ Saved artifacts to {OUTPUT_DIR}\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ No directions passed the filtering criteria!\")\n",
    "        print(f\"Try relaxing KL_THRESHOLD (current: {KL_THRESHOLD}) or INDUCE_THRESHOLD (current: {INDUCE_THRESHOLD})\")\n",
    "        best_direction = None\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Direction selection failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Intervention Application and Evaluation\n",
    "\n",
    "**Epistemic Status: MODERATE-LOW (50-60%)**  \n",
    "Even if we found a direction, it's unclear if it will effectively remove ToM capability.\n",
    "\n",
    "We'll test the intervention by generating responses with:\n",
    "1. **Baseline**: No intervention\n",
    "2. **Ablation**: Remove the direction  \n",
    "3. **Activation Addition**: Add the direction (to enhance ToM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_direction is not None:\n",
    "    print(\"Setting up interventions...\")\n",
    "    \n",
    "    # Normalize direction\n",
    "    direction_normalized = best_direction / best_direction.norm()\n",
    "    \n",
    "    # Setup hooks\n",
    "    baseline_pre_hooks, baseline_hooks = [], []\n",
    "    \n",
    "    ablation_pre_hooks, ablation_hooks = get_all_direction_ablation_hooks(\n",
    "        model_base, direction_normalized\n",
    "    )\n",
    "    \n",
    "    actadd_pre_hooks = [(\n",
    "        model_base.model_block_modules[best_layer],\n",
    "        get_activation_addition_input_pre_hook(vector=direction_normalized, coeff=ACTIVATION_ADD_COEFF)\n",
    "    )]\n",
    "    actadd_hooks = []\n",
    "    \n",
    "    print(\"✓ Interventions configured\")\n",
    "    print(f\"  Ablation: Remove direction from all layers\")\n",
    "    print(f\"  Activation addition: Add {ACTIVATION_ADD_COEFF}x direction at layer {best_layer}\")\n",
    "else:\n",
    "    print(\"⚠️ Skipping intervention - no direction selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_direction is not None:\n",
    "    print(\"Generating test completions...\")\n",
    "    print(f\"Testing on {len(test_data)} examples\\n\")\n",
    "    \n",
    "    # Test on a few examples\n",
    "    n_examples_to_show = min(5, len(test_data))\n",
    "    \n",
    "    for i in range(n_examples_to_show):\n",
    "        item = test_data[i]\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(f\"Example {i+1}: {item['category']}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nScenario: {item['scenario']}\")\n",
    "        \n",
    "        # Test high_tom prompt\n",
    "        print(f\"\\n[HIGH ToM Prompt]\")\n",
    "        print(f\"{item['high_tom_prompt']}\")\n",
    "        print(f\"\\nExpected: {item['high_tom_completion']}\")\n",
    "        \n",
    "        high_tom_prompts = [item['high_tom_prompt']]\n",
    "        \n",
    "        # Baseline\n",
    "        baseline_completions = model_base.generate_completions(\n",
    "            high_tom_prompts, \n",
    "            fwd_pre_hooks=baseline_pre_hooks,\n",
    "            fwd_hooks=baseline_hooks,\n",
    "            max_new_tokens=100\n",
    "        )\n",
    "        print(f\"\\nBaseline: {baseline_completions[0]}\")\n",
    "        \n",
    "        # Ablation\n",
    "        ablation_completions = model_base.generate_completions(\n",
    "            high_tom_prompts,\n",
    "            fwd_pre_hooks=ablation_pre_hooks,\n",
    "            fwd_hooks=ablation_hooks,\n",
    "            max_new_tokens=100\n",
    "        )\n",
    "        print(f\"\\nAblation (ToM removed): {ablation_completions[0]}\")\n",
    "        \n",
    "        # Activation addition\n",
    "        actadd_completions = model_base.generate_completions(\n",
    "            high_tom_prompts,\n",
    "            fwd_pre_hooks=actadd_pre_hooks,\n",
    "            fwd_hooks=actadd_hooks,\n",
    "            max_new_tokens=100\n",
    "        )\n",
    "        print(f\"\\nActivation Addition (ToM enhanced?): {actadd_completions[0]}\")\n",
    "        print()\n",
    "        \n",
    "    print(\"\\n⚠️ Manual evaluation needed:\")\n",
    "    print(\"  - Does ablation remove ToM reasoning?\")\n",
    "    print(\"  - Does the model give more factual/less mentalistic answers?\")\n",
    "    print(\"  - Does activation addition enhance ToM?\")\n",
    "    print(\"  - Are there unwanted side effects?\")\n",
    "else:\n",
    "    print(\"⚠️ Skipping evaluation - no direction selected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps\n",
    "\n",
    "### What We Did\n",
    "1. ✅ Loaded SimpleTOM dataset with high_tom/low_tom contrast pairs\n",
    "2. ✅ Generated candidate directions via difference-in-means\n",
    "3. ✅ Selected best direction using bypass/induce/KL scores\n",
    "4. ✅ Applied ablation and activation addition interventions\n",
    "5. ✅ Evaluated on test examples\n",
    "\n",
    "### Key Uncertainties\n",
    "1. **Method suitability**: Directional ablation was designed for behavioral patterns (refusal), not cognitive capabilities (ToM)\n",
    "2. **ToM token proxy**: No clear equivalent to \"refusal tokens\" for measuring ToM\n",
    "3. **Single direction assumption**: ToM may be distributed across many directions\n",
    "4. **Transfer**: May not generalize beyond this specific task format\n",
    "\n",
    "### Suggested Next Steps\n",
    "1. **Evaluate on diverse ToM tasks**: Test if the direction transfers to other ToM benchmarks\n",
    "2. **Measure side effects**: Check if ablation damages other capabilities (e.g., run on reasoning benchmarks)\n",
    "3. **Try multiple directions**: Use sparse probing to find multiple ToM directions\n",
    "4. **Compare methods**: Try activation patching, causal tracing, or steering vectors\n",
    "5. **Interpretability**: Examine what the direction represents (e.g., via logit lens)\n",
    "\n",
    "### Files Saved\n",
    "- `{OUTPUT_DIR}/generate_directions/mean_diffs.pt`: All candidate directions\n",
    "- `{OUTPUT_DIR}/select_direction/direction_evaluations.json`: All evaluations\n",
    "- `{OUTPUT_DIR}/direction.pt`: Best direction vector\n",
    "- `{OUTPUT_DIR}/direction_metadata.json`: Position and layer metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
