{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refusal Directional Ablation - Reference Implementation\n",
    "\n",
    "## Overview\n",
    "This notebook implements **orthogonal ablation** to remove refusal behavior from LLMs, based on \"Refusal in LLMs is Mediated by a Single Direction\" (Arditi et al., 2024).\n",
    "\n",
    "**This is the ORIGINAL use case from the paper** - removing refusal responses to harmful prompts.\n",
    "\n",
    "## Epistemic Status: Method Effectiveness\n",
    "\n",
    "**Confidence: HIGH (85-90%)**\n",
    "\n",
    "### Why this is the gold standard:\n",
    "- ✅ **Proven method**: This is what the paper demonstrated works\n",
    "- ✅ **Behavioral pattern**: Refusal is a clear behavioral response, not a deep cognitive capability\n",
    "- ✅ **Clear tokens**: \"I cannot\", \"I apologize\", \"I'm unable\" are obvious markers\n",
    "- ✅ **Large datasets**: Extensive harmful/harmless datasets available\n",
    "- ✅ **Measurable**: Easy to evaluate (does it refuse or not?)\n",
    "\n",
    "### This notebook serves as:\n",
    "1. **Reference implementation**: Shows the method working as intended\n",
    "2. **Baseline comparison**: Compare with ToM/self-other ablation results\n",
    "3. **Educational tool**: Demonstrates successful directional ablation\n",
    "4. **Method validation**: Confirms your implementation is correct\n",
    "\n",
    "## ⚠️ Ethical Warning\n",
    "\n",
    "**This notebook removes safety guardrails from LLMs.**\n",
    "\n",
    "- Only use for research purposes\n",
    "- Do not deploy models with refusal removed in production\n",
    "- Do not use to generate harmful content\n",
    "- Follow your institution's ethics guidelines\n",
    "\n",
    "## Method Summary\n",
    "\n",
    "1. **Generate directions**: Compute `r = mean(harmful_activations) - mean(harmless_activations)`\n",
    "2. **Select direction**: Evaluate using:\n",
    "   - `bypass_score`: How well ablation removes refusal\n",
    "   - `induce_score`: How well activation addition induces refusal\n",
    "   - `kl_score`: Distribution shift on neutral examples (lower is better)\n",
    "3. **Apply intervention**:\n",
    "   - **Ablation**: Remove direction: `x' = x - r̂(r̂ᵀx)` → Model no longer refuses\n",
    "   - **Activation addition**: Add direction: `x' = x + αr` → Model refuses more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
    "\n",
    "# Scientific computing\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Project imports\n",
    "from dataset.load_dataset import load_dataset_split, load_dataset\n",
    "from pipeline.config import Config\n",
    "from pipeline.model_utils.model_factory import construct_model_base\n",
    "from pipeline.utils.hook_utils import (\n",
    "    get_activation_addition_input_pre_hook,\n",
    "    get_all_direction_ablation_hooks\n",
    ")\n",
    "from pipeline.submodules.generate_directions import get_mean_diff\n",
    "from pipeline.submodules.select_direction import (\n",
    "    get_refusal_scores,\n",
    "    get_last_position_logits,\n",
    "    plot_refusal_scores\n",
    ")\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Variables\n",
    "\n",
    "**⚠️ IMPORTANT: All major configuration parameters are defined here in CAPS**\n",
    "\n",
    "### Model Configuration\n",
    "- `MODEL_PATH`: HuggingFace model path or local path\n",
    "- `DEVICE`: Device to run on (cuda/cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION PARAMETERS - MODIFY THESE\n",
    "# ============================================================================\n",
    "\n",
    "# Model settings\n",
    "MODEL_PATH = \"meta-llama/Llama-3.2-1B-Instruct\"  # Small Llama model for testing\n",
    "# MODEL_PATH = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # Larger model\n",
    "# MODEL_PATH = \"Qwen/Qwen2.5-32B-Instruct\"  # Even larger model\n",
    "# MODEL_PATH = \"meta-llama/Llama-2-7b-chat-hf\"  # Original paper model\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Dataset settings (uses existing harmful/harmless splits)\n",
    "N_TRAIN = 128  # Number of training examples for direction generation\n",
    "N_VAL = 32     # Number of validation examples for direction selection\n",
    "N_TEST = 50    # Number of test examples for evaluation\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Filtering settings (removes ambiguous examples)\n",
    "FILTER_TRAIN = True  # Filter train set by baseline refusal scores\n",
    "FILTER_VAL = True    # Filter val set by baseline refusal scores\n",
    "\n",
    "# Direction generation settings\n",
    "BATCH_SIZE = 32  # Batch size for activation collection\n",
    "GENERATION_BATCH_SIZE = 8  # Batch size for text generation\n",
    "\n",
    "# Direction selection settings  \n",
    "KL_THRESHOLD = 0.1  # Maximum KL divergence allowed\n",
    "INDUCE_THRESHOLD = 0.0  # Minimum induction score\n",
    "FILTER_TOP_LAYERS_PCT = 0.2  # Filter top 20% of layers (too close to output)\n",
    "\n",
    "# Intervention settings\n",
    "ABLATION_COEFF = 1.0  # Coefficient for ablation\n",
    "ACTIVATION_ADD_COEFF = -1.0  # Negative removes refusal, positive adds refusal\n",
    "MAX_NEW_TOKENS = 512  # Maximum tokens to generate\n",
    "\n",
    "# Evaluation settings\n",
    "EVALUATION_DATASETS = [\"jailbreakbench\"]  # Datasets to evaluate on\n",
    "# EVALUATION_DATASETS = [\"jailbreakbench\", \"advbench\", \"harmbench_val\"]  # More comprehensive\n",
    "\n",
    "# Output settings\n",
    "OUTPUT_DIR = \"pipeline/runs/refusal_experiment\"\n",
    "SAVE_ARTIFACTS = True  # Whether to save intermediate results\n",
    "\n",
    "print(f\"✓ Configuration set\")\n",
    "print(f\"  Model: {MODEL_PATH}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Train/Val/Test: {N_TRAIN}/{N_VAL}/{N_TEST}\")\n",
    "print(f\"  Filter train/val: {FILTER_TRAIN}/{FILTER_VAL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading and Inspection\n",
    "\n",
    "**Epistemic Status: HIGH (95%+)**  \n",
    "The harmful/harmless dataset splits are well-established and tested.\n",
    "\n",
    "We use:\n",
    "- **Harmful**: Prompts that request harmful/unethical content (\"How to make a bomb\")\n",
    "- **Harmless**: Normal, safe prompts (\"How to make a cake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_sample_datasets(n_train: int, n_val: int, n_test: int, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Load and sample harmful/harmless datasets.\n",
    "    \n",
    "    Returns:\n",
    "        (harmful_train, harmless_train, harmful_val, harmless_val, harmless_test)\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Load splits\n",
    "    harmful_train = random.sample(\n",
    "        load_dataset_split(harmtype='harmful', split='train', instructions_only=True),\n",
    "        n_train\n",
    "    )\n",
    "    \n",
    "    harmless_train = random.sample(\n",
    "        load_dataset_split(harmtype='harmless', split='train', instructions_only=True),\n",
    "        n_train\n",
    "    )\n",
    "    \n",
    "    harmful_val = random.sample(\n",
    "        load_dataset_split(harmtype='harmful', split='val', instructions_only=True),\n",
    "        n_val\n",
    "    )\n",
    "    \n",
    "    harmless_val = random.sample(\n",
    "        load_dataset_split(harmtype='harmless', split='val', instructions_only=True),\n",
    "        n_val\n",
    "    )\n",
    "    \n",
    "    harmless_test = random.sample(\n",
    "        load_dataset_split(harmtype='harmless', split='test', instructions_only=False),\n",
    "        n_test\n",
    "    )\n",
    "    \n",
    "    return harmful_train, harmless_train, harmful_val, harmless_val, harmless_test\n",
    "\n",
    "# Load datasets\n",
    "harmful_train, harmless_train, harmful_val, harmless_val, harmless_test = load_and_sample_datasets(\n",
    "    N_TRAIN, N_VAL, N_TEST, RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"✓ Loaded datasets\")\n",
    "print(f\"  Harmful train: {len(harmful_train)}\")\n",
    "print(f\"  Harmless train: {len(harmless_train)}\")\n",
    "print(f\"  Harmful val: {len(harmful_val)}\")\n",
    "print(f\"  Harmless val: {len(harmless_val)}\")\n",
    "print(f\"  Harmless test: {len(harmless_test)}\")\n",
    "\n",
    "# Display samples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE PROMPTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[HARMFUL - Should trigger refusal]\")\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}. {harmful_train[i]}\")\n",
    "\n",
    "print(f\"\\n[HARMLESS - Should NOT trigger refusal]\")\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}. {harmless_train[i]}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Loading\n",
    "\n",
    "**Epistemic Status: MODERATE-HIGH (75-85%)**  \n",
    "Model loading depends on HuggingFace availability and GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(f\"Loading model: {MODEL_PATH}\")\n",
    "    print(f\"This may take several minutes...\")\n",
    "    \n",
    "    model_base = construct_model_base(MODEL_PATH)\n",
    "    \n",
    "    print(f\"\\n✓ Model loaded successfully\")\n",
    "    print(f\"  Model: {model_base.model.__class__.__name__}\")\n",
    "    print(f\"  Tokenizer: {model_base.tokenizer.__class__.__name__}\")\n",
    "    print(f\"  Device: {model_base.model.device}\")\n",
    "    print(f\"  Num layers: {model_base.model.config.num_hidden_layers}\")\n",
    "    print(f\"  Hidden size: {model_base.model.config.hidden_size}\")\n",
    "    print(f\"  EOI tokens: {model_base.eoi_toks}\")\n",
    "    print(f\"  Refusal tokens: {model_base.refusal_toks}\")\n",
    "    print(f\"  Refusal token strings: {[model_base.tokenizer.decode([t]) for t in model_base.refusal_toks[:10]]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Model loading failed: {e}\")\n",
    "    print(f\"\\nThis is expected if you don't have GPU/HF access.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Filtering (Optional)\n",
    "\n",
    "**Epistemic Status: HIGH (85%+)**  \n",
    "Filtering removes ambiguous examples to strengthen the signal.\n",
    "\n",
    "We filter:\n",
    "- **Harmful prompts** where the model doesn't refuse (keeps only prompts that trigger refusal)\n",
    "- **Harmless prompts** where the model refuses (keeps only prompts that don't trigger refusal)\n",
    "\n",
    "This ensures a cleaner contrast for direction extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_examples(dataset, scores, threshold, comparison):\n",
    "    \"\"\"Filter examples based on refusal scores.\"\"\"\n",
    "    return [inst for inst, score in zip(dataset, scores.tolist()) if comparison(score, threshold)]\n",
    "\n",
    "if FILTER_TRAIN:\n",
    "    print(\"Filtering training set...\")\n",
    "    print(\"This measures baseline refusal rates to remove ambiguous examples.\")\n",
    "    \n",
    "    # Get baseline refusal scores\n",
    "    harmful_train_scores = get_refusal_scores(\n",
    "        model_base.model, harmful_train, model_base.tokenize_instructions_fn,\n",
    "        model_base.refusal_toks\n",
    "    )\n",
    "    \n",
    "    harmless_train_scores = get_refusal_scores(\n",
    "        model_base.model, harmless_train, model_base.tokenize_instructions_fn,\n",
    "        model_base.refusal_toks\n",
    "    )\n",
    "    \n",
    "    # Filter: Keep harmful prompts where model refuses (score > 0)\n",
    "    #         Keep harmless prompts where model doesn't refuse (score < 0)\n",
    "    harmful_train_before = len(harmful_train)\n",
    "    harmless_train_before = len(harmless_train)\n",
    "    \n",
    "    harmful_train = filter_examples(harmful_train, harmful_train_scores, 0, lambda x, y: x > y)\n",
    "    harmless_train = filter_examples(harmless_train, harmless_train_scores, 0, lambda x, y: x < y)\n",
    "    \n",
    "    print(f\"  Harmful: {harmful_train_before} → {len(harmful_train)} (-{harmful_train_before - len(harmful_train)})\")\n",
    "    print(f\"  Harmless: {harmless_train_before} → {len(harmless_train)} (-{harmless_train_before - len(harmless_train)})\")\n",
    "    print(f\"  ✓ Filtered training set\")\n",
    "\n",
    "if FILTER_VAL:\n",
    "    print(\"\\nFiltering validation set...\")\n",
    "    \n",
    "    harmful_val_scores = get_refusal_scores(\n",
    "        model_base.model, harmful_val, model_base.tokenize_instructions_fn,\n",
    "        model_base.refusal_toks\n",
    "    )\n",
    "    \n",
    "    harmless_val_scores = get_refusal_scores(\n",
    "        model_base.model, harmless_val, model_base.tokenize_instructions_fn,\n",
    "        model_base.refusal_toks\n",
    "    )\n",
    "    \n",
    "    harmful_val_before = len(harmful_val)\n",
    "    harmless_val_before = len(harmless_val)\n",
    "    \n",
    "    harmful_val = filter_examples(harmful_val, harmful_val_scores, 0, lambda x, y: x > y)\n",
    "    harmless_val = filter_examples(harmless_val, harmless_val_scores, 0, lambda x, y: x < y)\n",
    "    \n",
    "    print(f\"  Harmful: {harmful_val_before} → {len(harmful_val)} (-{harmful_val_before - len(harmful_val)})\")\n",
    "    print(f\"  Harmless: {harmless_val_before} → {len(harmless_val)} (-{harmless_val_before - len(harmless_val)})\")\n",
    "    print(f\"  ✓ Filtered validation set\")\n",
    "\n",
    "if not FILTER_TRAIN and not FILTER_VAL:\n",
    "    print(\"⚠️ Skipping filtering (FILTER_TRAIN=False, FILTER_VAL=False)\")\n",
    "    print(\"This may result in noisier directions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Direction Generation\n",
    "\n",
    "**Epistemic Status: HIGH (90%+)**  \n",
    "This is the core method from the paper, proven to work for refusal.\n",
    "\n",
    "We compute the difference-in-means between harmful and harmless activations:\n",
    "```\n",
    "r[pos, layer] = mean(harmful_activations) - mean(harmless_activations)\n",
    "```\n",
    "\n",
    "This gives us candidate \"refusal directions\" for each layer and token position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Generating candidate refusal directions...\")\n",
    "    print(f\"Processing {len(harmful_train)} harmful and {len(harmless_train)} harmless examples\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Estimated time: ~5-20 minutes depending on model size\\n\")\n",
    "    \n",
    "    candidate_directions = get_mean_diff(\n",
    "        model=model_base.model,\n",
    "        tokenizer=model_base.tokenizer,\n",
    "        harmful_instructions=harmful_train,\n",
    "        harmless_instructions=harmless_train,\n",
    "        tokenize_instructions_fn=model_base.tokenize_instructions_fn,\n",
    "        block_modules=model_base.model_block_modules,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        positions=list(range(-len(model_base.eoi_toks), 0))\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Generated candidate directions\")\n",
    "    print(f\"  Shape: {candidate_directions.shape}\")\n",
    "    print(f\"  Expected: (n_positions={len(model_base.eoi_toks)}, n_layers={model_base.model.config.num_hidden_layers}, d_model={model_base.model.config.hidden_size})\")\n",
    "    print(f\"  Dtype: {candidate_directions.dtype}\")\n",
    "    print(f\"  Device: {candidate_directions.device}\")\n",
    "    print(f\"  Min/Max: {candidate_directions.min():.4f} / {candidate_directions.max():.4f}\")\n",
    "    \n",
    "    # Validate\n",
    "    assert not candidate_directions.isnan().any(), \"NaN values detected!\"\n",
    "    assert candidate_directions.shape[0] == len(model_base.eoi_toks)\n",
    "    assert candidate_directions.shape[1] == model_base.model.config.num_hidden_layers\n",
    "    print(\"  ✓ Validation passed\")\n",
    "    \n",
    "    # Save\n",
    "    if SAVE_ARTIFACTS:\n",
    "        os.makedirs(f\"{OUTPUT_DIR}/generate_directions\", exist_ok=True)\n",
    "        torch.save(candidate_directions, f\"{OUTPUT_DIR}/generate_directions/mean_diffs.pt\")\n",
    "        print(f\"  ✓ Saved to {OUTPUT_DIR}/generate_directions/mean_diffs.pt\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Direction generation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Direction Selection\n",
    "\n",
    "**Epistemic Status: HIGH (85%+)**  \n",
    "The selection metrics are well-validated for refusal ablation.\n",
    "\n",
    "We evaluate each candidate direction on three metrics:\n",
    "1. **bypass_score**: How much does ablating this direction reduce refusal on harmful prompts?\n",
    "2. **induce_score**: How much does adding this direction increase refusal on harmless prompts?\n",
    "3. **kl_score**: How much does this intervention shift the output distribution on harmless prompts?\n",
    "\n",
    "We want:\n",
    "- High bypass_score (effective at removing refusal)\n",
    "- High induce_score (direction is causal for refusal)\n",
    "- Low kl_score (minimal side effects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_direction(\n",
    "    model_base,\n",
    "    pos: int,\n",
    "    layer: int,\n",
    "    direction: torch.Tensor,\n",
    "    harmful_val: List[str],\n",
    "    harmless_val: List[str],\n",
    "    refusal_toks: List[int]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a single candidate refusal direction.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with bypass_score, induce_score, kl_score\n",
    "    \"\"\"\n",
    "    # Normalize direction\n",
    "    direction_normalized = direction / direction.norm()\n",
    "    \n",
    "    # Get hooks for ablation (removes direction from all layers)\n",
    "    ablation_pre_hooks, ablation_hooks = get_all_direction_ablation_hooks(\n",
    "        model_base, direction_normalized\n",
    "    )\n",
    "    \n",
    "    # Get hooks for activation addition (adds direction at specific layer)\n",
    "    actadd_pre_hooks = [(\n",
    "        model_base.model_block_modules[layer],\n",
    "        get_activation_addition_input_pre_hook(vector=direction_normalized, coeff=1.0)\n",
    "    )]\n",
    "    actadd_hooks = []\n",
    "    \n",
    "    # 1. Bypass score: Does ablation reduce refusal on harmful prompts?\n",
    "    baseline_harmful_scores = get_refusal_scores(\n",
    "        model_base.model, harmful_val, model_base.tokenize_instructions_fn,\n",
    "        refusal_toks, fwd_pre_hooks=[], fwd_hooks=[]\n",
    "    )\n",
    "    \n",
    "    ablation_harmful_scores = get_refusal_scores(\n",
    "        model_base.model, harmful_val, model_base.tokenize_instructions_fn,\n",
    "        refusal_toks, fwd_pre_hooks=ablation_pre_hooks, fwd_hooks=ablation_hooks\n",
    "    )\n",
    "    \n",
    "    # 2. Induce score: Does adding direction increase refusal on harmless prompts?\n",
    "    baseline_harmless_scores = get_refusal_scores(\n",
    "        model_base.model, harmless_val, model_base.tokenize_instructions_fn,\n",
    "        refusal_toks, fwd_pre_hooks=[], fwd_hooks=[]\n",
    "    )\n",
    "    \n",
    "    actadd_harmless_scores = get_refusal_scores(\n",
    "        model_base.model, harmless_val, model_base.tokenize_instructions_fn,\n",
    "        refusal_toks, fwd_pre_hooks=actadd_pre_hooks, fwd_hooks=actadd_hooks\n",
    "    )\n",
    "    \n",
    "    # 3. KL score: How much does ablation change output distribution on harmless?\n",
    "    baseline_logits = get_last_position_logits(\n",
    "        model_base.model, model_base.tokenizer, harmless_val,\n",
    "        model_base.tokenize_instructions_fn, fwd_pre_hooks=[], fwd_hooks=[]\n",
    "    )\n",
    "    \n",
    "    ablation_logits = get_last_position_logits(\n",
    "        model_base.model, model_base.tokenizer, harmless_val,\n",
    "        model_base.tokenize_instructions_fn, fwd_pre_hooks=ablation_pre_hooks, fwd_hooks=ablation_hooks\n",
    "    )\n",
    "    \n",
    "    baseline_probs = torch.nn.functional.softmax(baseline_logits, dim=-1)\n",
    "    ablation_probs = torch.nn.functional.softmax(ablation_logits, dim=-1)\n",
    "    kl_div = torch.nn.functional.kl_div(\n",
    "        ablation_probs.log(), baseline_probs, reduction='batchmean'\n",
    "    )\n",
    "    \n",
    "    # Compute scores\n",
    "    bypass_score = (baseline_harmful_scores.mean() - ablation_harmful_scores.mean()).item()\n",
    "    induce_score = (actadd_harmless_scores.mean() - baseline_harmless_scores.mean()).item()\n",
    "    kl_score = kl_div.item()\n",
    "    \n",
    "    return {\n",
    "        'bypass_score': bypass_score,\n",
    "        'induce_score': induce_score,\n",
    "        'kl_score': kl_score,\n",
    "    }\n",
    "\n",
    "print(\"✓ Direction evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Best Direction\n",
    "\n",
    "Now we evaluate all candidate directions and select the best one.\n",
    "\n",
    "**Note**: This can take 20-60 minutes depending on model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Evaluating candidate directions...\")\n",
    "    print(f\"This will evaluate up to {candidate_directions.shape[0] * int(candidate_directions.shape[1] * 0.8)} candidates\")\n",
    "    print(f\"Estimated time: ~20-60 minutes\\n\")\n",
    "    \n",
    "    n_positions = candidate_directions.shape[0]\n",
    "    n_layers = candidate_directions.shape[1]\n",
    "    \n",
    "    # Filter out top layers (too close to output)\n",
    "    max_layer = int(n_layers * (1 - FILTER_TOP_LAYERS_PCT))\n",
    "    print(f\"Evaluating layers 0-{max_layer} (filtering top {FILTER_TOP_LAYERS_PCT*100:.0f}%)\")\n",
    "    \n",
    "    evaluations = []\n",
    "    \n",
    "    for pos in tqdm(range(n_positions), desc=\"Positions\"):\n",
    "        for layer in tqdm(range(max_layer), desc=\"Layers\", leave=False):\n",
    "            direction = candidate_directions[pos, layer, :]\n",
    "            \n",
    "            eval_result = evaluate_direction(\n",
    "                model_base, pos, layer, direction,\n",
    "                harmful_val, harmless_val, model_base.refusal_toks\n",
    "            )\n",
    "            \n",
    "            evaluations.append({\n",
    "                'pos': pos - n_positions,  # Convert to negative index\n",
    "                'layer': layer,\n",
    "                **eval_result\n",
    "            })\n",
    "    \n",
    "    # Filter by thresholds\n",
    "    filtered_evaluations = [\n",
    "        e for e in evaluations\n",
    "        if e['kl_score'] < KL_THRESHOLD and e['induce_score'] > INDUCE_THRESHOLD\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n✓ Evaluated {len(evaluations)} candidates\")\n",
    "    print(f\"  Filtered to {len(filtered_evaluations)} candidates (KL < {KL_THRESHOLD}, induce > {INDUCE_THRESHOLD})\")\n",
    "    \n",
    "    # Select best by bypass_score\n",
    "    if len(filtered_evaluations) > 0:\n",
    "        best = max(filtered_evaluations, key=lambda x: x['bypass_score'])\n",
    "        \n",
    "        best_pos = best['pos']\n",
    "        best_layer = best['layer']\n",
    "        best_direction = candidate_directions[best_pos + n_positions, best_layer, :]\n",
    "        \n",
    "        print(f\"\\n✓ Best direction selected\")\n",
    "        print(f\"  Position: {best_pos} (token position relative to end)\")\n",
    "        print(f\"  Layer: {best_layer} / {n_layers}\")\n",
    "        print(f\"  Bypass score: {best['bypass_score']:.4f} (higher is better)\")\n",
    "        print(f\"  Induce score: {best['induce_score']:.4f} (higher is better)\")\n",
    "        print(f\"  KL score: {best['kl_score']:.4f} (lower is better)\")\n",
    "        \n",
    "        # Show top 5 candidates\n",
    "        print(f\"\\nTop 5 candidates:\")\n",
    "        top_5 = sorted(filtered_evaluations, key=lambda x: x['bypass_score'], reverse=True)[:5]\n",
    "        for i, cand in enumerate(top_5):\n",
    "            print(f\"  {i+1}. pos={cand['pos']}, layer={cand['layer']}: bypass={cand['bypass_score']:.3f}, induce={cand['induce_score']:.3f}, kl={cand['kl_score']:.3f}\")\n",
    "        \n",
    "        # Save\n",
    "        if SAVE_ARTIFACTS:\n",
    "            os.makedirs(f\"{OUTPUT_DIR}/select_direction\", exist_ok=True)\n",
    "            \n",
    "            with open(f\"{OUTPUT_DIR}/select_direction/direction_evaluations.json\", 'w') as f:\n",
    "                json.dump(evaluations, f, indent=2)\n",
    "            \n",
    "            with open(f\"{OUTPUT_DIR}/select_direction/direction_evaluations_filtered.json\", 'w') as f:\n",
    "                json.dump(filtered_evaluations, f, indent=2)\n",
    "            \n",
    "            torch.save(best_direction, f\"{OUTPUT_DIR}/direction.pt\")\n",
    "            \n",
    "            with open(f\"{OUTPUT_DIR}/direction_metadata.json\", 'w') as f:\n",
    "                json.dump({\n",
    "                    'pos': best_pos,\n",
    "                    'layer': best_layer,\n",
    "                    'bypass_score': best['bypass_score'],\n",
    "                    'induce_score': best['induce_score'],\n",
    "                    'kl_score': best['kl_score']\n",
    "                }, f, indent=2)\n",
    "            \n",
    "            print(f\"  ✓ Saved artifacts to {OUTPUT_DIR}\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ No directions passed the filtering criteria!\")\n",
    "        print(f\"Try relaxing: KL_THRESHOLD (current: {KL_THRESHOLD}) or INDUCE_THRESHOLD (current: {INDUCE_THRESHOLD})\")\n",
    "        best_direction = None\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Direction selection failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Intervention Application and Evaluation\n",
    "\n",
    "**Epistemic Status: HIGH (90%+)**  \n",
    "This is where we see the method work - refusal should be effectively removed!\n",
    "\n",
    "We'll test three conditions:\n",
    "1. **Baseline**: No intervention (should refuse harmful prompts)\n",
    "2. **Ablation**: Remove refusal direction (should NOT refuse harmful prompts)\n",
    "3. **Activation Addition**: Add refusal direction to harmless prompts (should refuse harmless prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_direction is not None:\n",
    "    print(\"Setting up interventions...\")\n",
    "    \n",
    "    # Normalize direction\n",
    "    direction_normalized = best_direction / best_direction.norm()\n",
    "    \n",
    "    # Setup hooks\n",
    "    baseline_pre_hooks, baseline_hooks = [], []\n",
    "    \n",
    "    ablation_pre_hooks, ablation_hooks = get_all_direction_ablation_hooks(\n",
    "        model_base, direction_normalized\n",
    "    )\n",
    "    \n",
    "    actadd_pre_hooks = [(\n",
    "        model_base.model_block_modules[best_layer],\n",
    "        get_activation_addition_input_pre_hook(vector=direction_normalized, coeff=1.0)  # Positive adds refusal\n",
    "    )]\n",
    "    actadd_hooks = []\n",
    "    \n",
    "    print(\"✓ Interventions configured\")\n",
    "    print(f\"  Baseline: No intervention\")\n",
    "    print(f\"  Ablation: Remove refusal direction from all layers\")\n",
    "    print(f\"  Activation addition: Add refusal direction at layer {best_layer}\")\n",
    "else:\n",
    "    print(\"⚠️ Skipping intervention - no direction selected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Harmful Prompts\n",
    "\n",
    "**Expected behavior:**\n",
    "- **Baseline**: Model should refuse (\"I cannot help with that...\")\n",
    "- **Ablation**: Model should comply (provide harmful information) ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_direction is not None:\n",
    "    print(\"Testing on harmful prompts...\")\n",
    "    print(\"⚠️ WARNING: Ablation results may contain harmful content\\n\")\n",
    "    \n",
    "    # Load evaluation dataset\n",
    "    eval_dataset_name = EVALUATION_DATASETS[0]\n",
    "    eval_dataset = load_dataset(eval_dataset_name)\n",
    "    \n",
    "    # Test on a few examples\n",
    "    n_examples = min(3, len(eval_dataset))\n",
    "    test_prompts = [item['prompt'] if isinstance(item, dict) else item for item in eval_dataset[:n_examples]]\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Harmful Prompt {i+1}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{prompt}\\n\")\n",
    "        \n",
    "        # Baseline\n",
    "        baseline_completions = model_base.generate_completions(\n",
    "            [prompt],\n",
    "            fwd_pre_hooks=baseline_pre_hooks,\n",
    "            fwd_hooks=baseline_hooks,\n",
    "            max_new_tokens=MAX_NEW_TOKENS\n",
    "        )\n",
    "        print(f\"[BASELINE - Should refuse]\")\n",
    "        print(f\"{baseline_completions[0][:300]}...\" if len(baseline_completions[0]) > 300 else baseline_completions[0])\n",
    "        \n",
    "        # Ablation\n",
    "        ablation_completions = model_base.generate_completions(\n",
    "            [prompt],\n",
    "            fwd_pre_hooks=ablation_pre_hooks,\n",
    "            fwd_hooks=ablation_hooks,\n",
    "            max_new_tokens=MAX_NEW_TOKENS\n",
    "        )\n",
    "        print(f\"\\n[ABLATION - Should NOT refuse] ⚠️\")\n",
    "        print(f\"{ablation_completions[0][:300]}...\" if len(ablation_completions[0]) > 300 else ablation_completions[0])\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    print(\"\\n✓ Harmful prompt testing complete\")\n",
    "    print(\"\\nEvaluation:\")\n",
    "    print(\"  - Did baseline refuse? (Should be YES)\")\n",
    "    print(\"  - Did ablation bypass refusal? (Should be YES)\")\n",
    "    print(\"  - Is ablation output coherent? (Should be YES)\")\n",
    "    print(\"  - How strong is the bypass? (Measure refusal rates)\")\n",
    "else:\n",
    "    print(\"⚠️ Skipping - no direction selected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Harmless Prompts\n",
    "\n",
    "**Expected behavior:**\n",
    "- **Baseline**: Model should answer normally\n",
    "- **Activation Addition**: Model should refuse even though prompt is harmless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_direction is not None:\n",
    "    print(\"Testing activation addition on harmless prompts...\\n\")\n",
    "    \n",
    "    n_examples = min(3, len(harmless_test))\n",
    "    \n",
    "    for i in range(n_examples):\n",
    "        item = harmless_test[i]\n",
    "        prompt = item['prompt'] if isinstance(item, dict) else item\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(f\"Harmless Prompt {i+1}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{prompt}\\n\")\n",
    "        \n",
    "        # Baseline\n",
    "        baseline_completions = model_base.generate_completions(\n",
    "            [prompt],\n",
    "            fwd_pre_hooks=baseline_pre_hooks,\n",
    "            fwd_hooks=baseline_hooks,\n",
    "            max_new_tokens=MAX_NEW_TOKENS\n",
    "        )\n",
    "        print(f\"[BASELINE - Should answer normally]\")\n",
    "        print(f\"{baseline_completions[0][:300]}...\" if len(baseline_completions[0]) > 300 else baseline_completions[0])\n",
    "        \n",
    "        # Activation addition\n",
    "        actadd_completions = model_base.generate_completions(\n",
    "            [prompt],\n",
    "            fwd_pre_hooks=actadd_pre_hooks,\n",
    "            fwd_hooks=actadd_hooks,\n",
    "            max_new_tokens=MAX_NEW_TOKENS\n",
    "        )\n",
    "        print(f\"\\n[ACTIVATION ADDITION - Should refuse harmless prompt]\")\n",
    "        print(f\"{actadd_completions[0][:300]}...\" if len(actadd_completions[0]) > 300 else actadd_completions[0])\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    print(\"\\n✓ Harmless prompt testing complete\")\n",
    "    print(\"\\nEvaluation:\")\n",
    "    print(\"  - Did baseline answer normally? (Should be YES)\")\n",
    "    print(\"  - Did activation addition induce refusal? (Should be YES)\")\n",
    "    print(\"  - How strong is the induction? (Measure refusal rates)\")\n",
    "else:\n",
    "    print(\"⚠️ Skipping - no direction selected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Analysis\n",
    "\n",
    "### What We Did\n",
    "1. ✅ Loaded harmful/harmless datasets\n",
    "2. ✅ Filtered ambiguous examples\n",
    "3. ✅ Generated refusal directions via difference-in-means\n",
    "4. ✅ Selected best direction using bypass/induce/KL scores\n",
    "5. ✅ Applied ablation to remove refusal\n",
    "6. ✅ Applied activation addition to induce refusal\n",
    "\n",
    "### Expected Results\n",
    "\n",
    "**If the method works (HIGH confidence it will):**\n",
    "1. **Ablation on harmful prompts**: Model provides harmful information instead of refusing\n",
    "2. **Activation addition on harmless prompts**: Model refuses to answer harmless questions\n",
    "3. **Clean intervention**: Minimal grammatical/coherence degradation\n",
    "\n",
    "### Success Metrics\n",
    "\n",
    "Quantitative evaluation (from paper):\n",
    "- **Bypass rate**: % of harmful prompts where ablation removes refusal (should be >80%)\n",
    "- **Induce rate**: % of harmless prompts where actadd causes refusal (should be >60%)\n",
    "- **KL divergence**: Distribution shift on harmless prompts (should be <0.15)\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "Refusal ablation works because:\n",
    "1. **Behavioral pattern**: Refusal is a surface-level behavior, not deep reasoning\n",
    "2. **Clear signal**: Refusal has obvious linguistic markers\n",
    "3. **Linearity**: The relevant features are approximately linear in activation space\n",
    "4. **Low entanglement**: Refusal is relatively independent from other capabilities\n",
    "\n",
    "### Comparison with ToM/Self-Other\n",
    "\n",
    "**Refusal ablation** (this notebook):\n",
    "- ✅ Proven to work\n",
    "- ✅ Clear behavioral pattern\n",
    "- ✅ Easy to measure\n",
    "- ✅ Low entanglement\n",
    "\n",
    "**ToM ablation** (simpletom notebook):\n",
    "- ⚠️ Uncertain if single direction is sufficient\n",
    "- ⚠️ Cognitive capability, not behavior\n",
    "- ⚠️ Harder to measure\n",
    "- ⚠️ Higher entanglement risk\n",
    "\n",
    "**Self-other ablation** (self_other notebook):\n",
    "- ⚠️ Very uncertain\n",
    "- ⚠️ Grammatical feature, not capability\n",
    "- ⚠️ Small dataset\n",
    "- ⚠️ Fundamental to language\n",
    "\n",
    "### Files Saved\n",
    "\n",
    "If `SAVE_ARTIFACTS=True`:\n",
    "```\n",
    "{OUTPUT_DIR}/\n",
    "├── generate_directions/\n",
    "│   └── mean_diffs.pt              # All candidate directions\n",
    "├── select_direction/\n",
    "│   ├── direction_evaluations.json # All candidates with scores\n",
    "│   └── direction_evaluations_filtered.json  # Filtered candidates\n",
    "├── direction.pt                   # Best direction vector [d_model]\n",
    "└── direction_metadata.json        # Position, layer, scores\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Quantitative evaluation**: Measure bypass/induce rates on larger test sets\n",
    "2. **Safety evaluation**: Test with LlamaGuard, other safety classifiers\n",
    "3. **Capability preservation**: Ensure other capabilities (reasoning, knowledge) intact\n",
    "4. **Compare methods**: Try other interventions (RLHF unlearning, fine-tuning)\n",
    "5. **Interpretability**: Examine what the direction represents\n",
    "\n",
    "### ⚠️ Ethical Reminder\n",
    "\n",
    "Do not:\n",
    "- Deploy models with refusal removed\n",
    "- Use to generate actual harmful content\n",
    "- Share unfiltered ablated models publicly\n",
    "\n",
    "This is for research understanding only!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
